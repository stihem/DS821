{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "a8f3fabd-cbce-4750-9c43-d1691cb49a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from tabulate import tabulate\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "601c05de-b536-4dc1-93de-6269a705f898",
   "metadata": {},
   "source": [
    "The dataset is imported from Kaggle: https://www.kaggle.com/datasets/gpreda/reddit-wallstreetsbets-post\n",
    "\n",
    "Upon inspecting the data structure in the source material, I learned that it comprise of eight columns: title, score, id, url, comms_num, created, body and timestamp.\n",
    "\n",
    "The original dataset is aprox. 37MB, wich is too big to upload on GitHub. So for the purpose of cleaning and there by minimizing the file, the columns score, id, url, comms_num and created was removed before loading the data. Since the file was still too big, it needed to be split in two parts in order to be able to upload them to a GitHub repository.\n",
    "\n",
    "The title of the files are cleaned_reddit_wsb_part1.csv and cleaned_reddit_wsb_part2.csv."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb0f5b5-1536-4536-900f-e772aaef7fc5",
   "metadata": {},
   "source": [
    "Linking to a GitHub repository:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "f64c45dc-518d-4190-b226-8cb1380301a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repository already exists.\n"
     ]
    }
   ],
   "source": [
    "# GitHub repository URL and local path\n",
    "repo_url = \"https://github.com/stihem/DS821\"\n",
    "local_repo_path = os.path.join(os.getcwd(), \"DS821\")\n",
    "\n",
    "# Clone the repository if it doesn't exist\n",
    "if not os.path.exists(local_repo_path):\n",
    "    os.system(f\"git clone {repo_url} {local_repo_path}\")\n",
    "    print(f\"Repository cloned to {local_repo_path}\")\n",
    "else:\n",
    "    print(\"Repository already exists.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad4e1e85-0979-4751-a15b-90901a9b6cbd",
   "metadata": {},
   "source": [
    "Ajusting file paths and combining the two parts of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "562d71b1-575c-4afd-8625-0ae0716a5e47",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "One or more dataset parts are missing.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[69], line 11\u001b[0m\n\u001b[0;32m      6\u001b[0m     combined_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat(\n\u001b[0;32m      7\u001b[0m         [pd\u001b[38;5;241m.\u001b[39mread_csv(fp, parse_dates\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtimestamp\u001b[39m\u001b[38;5;124m'\u001b[39m]) \u001b[38;5;28;01mfor\u001b[39;00m fp \u001b[38;5;129;01min\u001b[39;00m file_paths], \n\u001b[0;32m      8\u001b[0m         ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m      9\u001b[0m     )\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 11\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOne or more dataset parts are missing.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: One or more dataset parts are missing."
     ]
    }
   ],
   "source": [
    "# File paths for dataset parts\n",
    "file_paths = [os.path.join(local_repo_path, f\"cleaned_reddit_wsb_part{i}.csv\") for i in [1, 2]]\n",
    "\n",
    "# Check if files exist and combine them\n",
    "if all(os.path.exists(fp) for fp in file_paths):\n",
    "    combined_df = pd.concat(\n",
    "        [pd.read_csv(fp, parse_dates=['timestamp']) for fp in file_paths], \n",
    "        ignore_index=True\n",
    "    )\n",
    "else:\n",
    "    raise FileNotFoundError(\"One or more dataset parts are missing.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05a04c60-e681-46b8-aa60-ff859efcd73c",
   "metadata": {},
   "source": [
    "Simplyfiing the data by combining the text in the title and body columns for later analysis and converting the timestamp to date only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d222b2f-358c-4fd0-b329-38787fc9e20d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check required columns and preprocess the data\n",
    "required_columns = {'timestamp', 'title', 'body'}\n",
    "if required_columns.issubset(combined_df.columns):\n",
    "    combined_df = combined_df[list(required_columns)].fillna('')\n",
    "    combined_df['full_text'] = combined_df['title'].str[:50] + '...' + combined_df['body'].str[:50]\n",
    "    combined_df['timestamp'] = pd.to_datetime(combined_df['timestamp']).dt.date\n",
    "    combined_df = combined_df[combined_df['title'] != '']\n",
    "    combined_df = combined_df.sort_values(by='timestamp')\n",
    "    \n",
    "    print(\"Data Preview:\")\n",
    "    display_df = combined_df[['timestamp', 'full_text']].head(10)\n",
    "    print(tabulate(display_df, headers='keys', tablefmt='fancy_grid', showindex=False))\n",
    "else:\n",
    "    raise ValueError(\"Missing one or more required columns in the dataset.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2397b21-37e1-4982-ab53-15d8a0f43397",
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Aggregate data: count the number of posts per date\n",
    "    post_counts = combined_df.groupby('timestamp').size()\n",
    "\n",
    "    # Plot the data with date on the x-axis\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(post_counts.index, post_counts.values, marker='o', linestyle='-', color='green', label='Number of Posts')\n",
    "    plt.fill_between(post_counts.index, post_counts.values, color='green', alpha=0.2)  # Optional area shading\n",
    "\n",
    "    # Format the plot\n",
    "    plt.title('Number of Posts Over Time', fontsize=16)\n",
    "    plt.xlabel('Date', fontsize=14)\n",
    "    plt.ylabel('Number of Posts', fontsize=14)\n",
    "    plt.xticks(rotation=45, fontsize=10)\n",
    "    plt.yticks(fontsize=10)\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "    plt.legend(fontsize=12)\n",
    "\n",
    "    # Show the plot\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc38af6-2964-4465-81d0-75c6c04de1d0",
   "metadata": {},
   "source": [
    "This plot highlights a single post dated before 2021-21, which will be removed to create a more streamlined dataset. The plot also shows that the highsest concentration of post is around 202102 so going forward the data will be limited to the first two weeks (2021-01-28 to 2021-02-10) in order to condense the dataset for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae178d76-995f-45df-853e-f1f7d92ffef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if required columns exist\n",
    "if 'timestamp' in combined_df.columns and 'body' in combined_df.columns:\n",
    "    # Reuse cleaned data: ensure 'full_text' column exists\n",
    "    if 'full_text' not in combined_df.columns:\n",
    "        combined_df = combined_df[['timestamp', 'body']].fillna('')\n",
    "        combined_df['full_text'] = combined_df['body'].str[:50]  # Use truncated 'body' for full_text\n",
    "        combined_df['timestamp'] = pd.to_datetime(combined_df['timestamp']).dt.date\n",
    "        combined_df = combined_df[combined_df['body'] != '']  # Remove rows with empty 'body'\n",
    "\n",
    "    # Aggregate data: count the number of posts per date\n",
    "    post_counts = combined_df.groupby('timestamp').size()\n",
    "\n",
    "    # Filter data to the specified date range\n",
    "    start_date = pd.to_datetime('2021-01-28').date()\n",
    "    end_date = pd.to_datetime('2021-02-10').date()\n",
    "    filtered_post_counts = post_counts.loc[start_date:end_date]\n",
    "\n",
    "    # Check if filtered data is empty\n",
    "    if filtered_post_counts.empty:\n",
    "        print(\"No posts found within the specified date range.\")\n",
    "    else:\n",
    "        # Plot the filtered data\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.plot(filtered_post_counts.index, filtered_post_counts.values, marker='o', linestyle='-', \n",
    "                 color='green', label='Number of Posts')\n",
    "        plt.fill_between(filtered_post_counts.index, filtered_post_counts.values, color='green', alpha=0.2)\n",
    "\n",
    "        # Update the plot title to reflect the filtering\n",
    "        plt.title('Number of Posts Over Time (Filtered by Date)', fontsize=16)\n",
    "        plt.xlabel('Date', fontsize=14)\n",
    "        plt.ylabel('Number of Posts', fontsize=14)\n",
    "        plt.xticks(rotation=45, fontsize=10)\n",
    "        plt.yticks(fontsize=10)\n",
    "        plt.grid(True, linestyle='--', alpha=0.7)\n",
    "        plt.legend(fontsize=12)\n",
    "\n",
    "        # Finalize and show the plot\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        # Save the filtered data into a list of dictionaries\n",
    "        filtered_data = combined_df[(combined_df['timestamp'] >= start_date) & \n",
    "                                    (combined_df['timestamp'] <= end_date)]\n",
    "        filtered_list = filtered_data[['timestamp', 'full_text']].to_dict(orient='records')\n",
    "\n",
    "        # Confirmation message\n",
    "        print(\"Filtered data has been saved in a list of dictionaries.\")\n",
    "        print(f\"Number of records: {len(filtered_list)}\")\n",
    "\n",
    "else:\n",
    "    print(\"Missing one or more required columns ('timestamp' or 'body').\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b0fa625-4282-4252-a56f-0624bac1cef6",
   "metadata": {},
   "source": [
    "Before using the NLP tools of NER and AFINN to determine if the chat on reddit has any relation with the performance on the stock market, the data needs to be cleaned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e3c4d8-a960-4c1a-98e1-fac62247e349",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
